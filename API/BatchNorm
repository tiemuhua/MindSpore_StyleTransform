>>> help("mindspore.nn.layer.normalization.BatchNorm2d")
Help on class BatchNorm2d in mindspore.nn.layer.normalization:

mindspore.nn.layer.normalization.BatchNorm2d = class BatchNorm2d(_BatchNorm)
 |  mindspore.nn.layer.normalization.BatchNorm2d(num_features, eps=1e-05, momentum=0.9, affine=True, gamma_init='ones', beta_init='zeros', moving_mean_init='zeros', moving_var_init='ones'
, use_batch_statistics=None, data_format='NCHW')
 |
 |  Batch normalization layer over a 4D input.
 |
 |  Batch Normalization is widely used in convolutional networks. This layer
 |  applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with
 |  additional channel dimension) to avoid internal covariate shift as described
 |  in the paper `Batch Normalization: Accelerating Deep Network Training by
 |  Reducing Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`_. It
 |  rescales and recenters the feature using a mini-batch of data and
 |  the learned parameters which can be described in the following formula.
 |
 |  .. math::
 |      y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta
 |
 |  Note:
 |      The implementation of BatchNorm is different in graph mode and pynative mode, therefore that mode can not be
 |      changed after net was initialized.
 |      Note that the formula for updating the running_mean and running_var is
 |      :math:`\hat{x}_\text{new} = (1 - \text{momentum}) \times x_t + \text{momentum} \times \hat{x}`,
 |      where :math:`\hat{x}` is the estimated statistic and :math:`x_t` is the new observed value.
 |
 |  Args:
 |      num_features (int): `C` from an expected input of size (N, C, H, W).
 |      eps (float): A value added to the denominator for numerical stability. Default: 1e-5.
 |      momentum (float): A floating hyperparameter of the momentum for the
 |          running_mean and running_var computation. Default: 0.9.
 |      affine (bool): A bool value. When set to True, gamma and beta can be learned. Default: True.
 |      gamma_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the gamma weight.
 |          The values of str refer to the function `initializer` including 'zeros', 'ones', 'xavier_uniform',
 |          'he_uniform', etc. Default: 'ones'.
 |      beta_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the beta weight.
 |          The values of str refer to the function `initializer` including 'zeros', 'ones', 'xavier_uniform',
 |          'he_uniform', etc. Default: 'zeros'.
 |      moving_mean_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the moving mean.
 |          The values of str refer to the function `initializer` including 'zeros', 'ones', 'xavier_uniform',
 |          'he_uniform', etc. Default: 'zeros'.
 |      moving_var_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the moving variance.
 |          The values of str refer to the function `initializer` including 'zeros', 'ones', 'xavier_uniform',
 |          'he_uniform', etc. Default: 'ones'.
 |      use_batch_statistics (bool): If true, use the mean value and variance value of current batch data. If false,
 |          use the mean value and variance value of specified value. If None, the training process will use the mean
 |          and variance of current batch data and track the running mean and variance, the evaluation process will use
 |          the running mean and variance. Default: None.
 |      data_format (str): The optional value for data format, is 'NHWC' or 'NCHW'.
 |          Default: 'NCHW'.
 |
 |  Inputs:
 |      - **input** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.
 |
 |  Outputs:
 |      Tensor, the normalized, scaled, offset tensor, of shape :math:`(N, C_{out}, H_{out}, W_{out})`.
 |
 |  Supported Platforms:
 |      ``Ascend`` ``GPU`` ``CPU``
 |
 |  Examples:
 |      >>> net = nn.BatchNorm2d(num_features=3)
 |      >>> np.random.seed(0)
 |      >>> input = Tensor(np.random.randint(0, 255, [1, 3, 2, 2]), mindspore.float32)
 |      >>> output = net(input)
 |      >>> print(output)
 |      [[[[171.99915   46.999763 ]
 |         [116.99941  191.99904  ]]
 |        [[ 66.999664 250.99875  ]
 |         [194.99902  102.99948  ]]
 |        [[  8.999955 210.99895  ]
 |         [ 20.999895 241.9988   ]]]]
